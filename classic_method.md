# Classic Recommendation Model

## 协同过滤

### userCF
共现矩阵中的行向量代表相应用户的用户向量，通过计算两用户向量的相似度（余弦相似度，皮尔逊相似度）来得到与某一用户最相似的n个用户，再利用用户相似度与相似用户对该商品评价的加权平均获得目标用户的评价预测。

缺点：
1. 需要维护用户相似度矩阵，而用户数通常远大于商品数，导致用户相似度矩阵存储开销很大。
2. 用户历史数据向量往往非常稀疏，不适合一些获取反馈较困难的场景。

### itemCF
通过计算共现矩阵两两列向量（即物品向量）之间的相似性，构建物品相似度矩阵。针对用户历史行为中正反馈的物品，找出相似top k的物品。

userCF 和 itemCF：
1. userCF 具有更强的社交特性，有发现热点，跟踪热点的趋势，适用于新闻推荐场景。
2. itemCF 更适用于兴趣变化较为稳定的场景。

CF缺陷：
1. 热门商品有很强的头部效应，与大量商品产生相似性。
2. 处理稀疏向量能力弱，导致尾部商品很少被推荐。
3. 无法引入其他信息，如用户性别，年龄，商品类别，价格，上下文特征等。
   
### 矩阵分解
矩阵分解再协同过滤的共现矩阵基础上，引入了隐向量概念，加强了模型处理稀疏矩阵的能力。

矩阵分解将为每一个用户和物品生成一个隐向量，距离相近的物品向量推荐给用户。而用户和物品的隐向量是通过分解协同过滤的共现矩阵得到的。

对矩阵进行分解的方法有三种：
1. 特征值分解：只能用于方阵，这里不适用。
2. 奇异值分解
3. 梯度下降

奇异值分解可以解决矩阵分解问题，但存在一些缺陷：
1. 奇异值分解要求原始矩阵是稠密的。
2. 奇异值分解计算复杂度很高。

因此，奇异值分解并不适用于大规模的矩阵分解问题。因此，梯度下降成了矩阵分解的主要方法。（主要通过让原始评分$r_ui$与用户向量和物品向量之积$q_i^Tp_u$的差值最小。）

矩阵分解的优点和缺陷：
1. 泛化能力强：一定程度上解决了数据稀疏问题。
2. 空间复杂度低，不需要存储庞大的用户相似性矩阵或物品相似性矩阵。
3. 与深度学习的embedding思想相似，方便与深度网络结合。
4. 但同样不方便引入其他用户，物品，上下文特征。

## 逻辑回归

逻辑回归数学表达式为：

$$f(x)=\frac{1}{1+e^{-(wx+b)}}$$

将正样本表示为1，负样本表示为0，则有：

$$P(y=1|x;w)=f(x)$$
$$P(y=0|x;w)=1-f(x)$$

综合起来，可以写成：
$$P(y|x;w) = (f(x))^y(1-f(x))^{1-y} $$

由极大似然估计可得逻辑回归的目标函数：
$$L(w) = \prod^n P(y|x;w) $$

由于目标函数是连乘形式，不方便求导，可两边取log，并乘以-(1/n)，从而转为求极小值问题：
$$J(w) = -\frac{1}{n}logL(w)=-\frac{1}{n}(\sum^n (ylogf(x)+(1-y)log(1-f(x))))$$

得到目标函数后，对每个参数求偏导，得到梯度方向：
$$\frac{\partial}{\partial w_j}J(w)=\frac{1}{n}\sum^n (f(x)-y)x_j $$

**优点**：
1. 数学含义上的支撑
2. 可解释性强
3. 易于工程化：可并行，模型简单，训练开销小。

**局限性**：
1. 表达能力较弱
2. 无法进行特征交叉，特征筛选等操作

## FM

为了实现特征交叉，一种直观的想法如下：
$$f(w, x) = \sum_i^n \sum_{j\neq i}^n w_{i, j}x_i x_j $$

这会导致原本就稀疏的特征向量更加稀疏，即大部分交叉特征的权重缺乏有效数据进行训练，无法收敛。同时权重参数w由n上升到$n^2$。

FM所做出的改进就是用两个向量的内积取代了单一的权重系数：FM为每一个特征学习了一个隐权重向量，在特征交叉时，使用两个特征隐向量的内积作为交叉特征的权重：
$$ FM(w, x)=\sum_i^n \sum_{j\neq i}^n (w_i w_j)x_i x_j$$

优点：
1. 将参数从$n^2$ 降到 $kn$， k为隐向量维度。
2. 更好解决数据稀疏性问题。如A 的隐向量可以通过(A,B)样本来更新，C的隐向量可以通过 (C, D)样本更新。这样只用(A,B),(C,D)样本就能拟合A和C的交叉特征权重。

### FFM
> field-aware FM

区别在于每个特征对应的不是唯一一个隐向量，而是一组隐向量。当与不同特征交叉时，会从这一组向量中挑出对应域的那个向量进行交叉。





