# Ensemble Learning

## 1. Boosting

Boosting è®­ç»ƒåŸºåˆ†ç±»å™¨é‡‡ç”¨ä¸²è¡Œæ–¹å¼ï¼Œå„ä¸ªåŸºåˆ†ç±»å™¨ä¹‹é—´æœ‰ä¾èµ–ã€‚

å°†åˆ†ç±»å™¨å±‚å±‚å åŠ ï¼Œæ¯ä¸€å±‚è®­ç»ƒæ—¶å¯¹å‰ä¸€å±‚åˆ†ç±»å™¨åˆ†é”™çš„æ ·æœ¬ç»™äºˆæ›´é«˜çš„æƒé‡ã€‚æµ‹è¯•æ—¶ï¼Œæ ¹æ®å„å±‚åˆ†ç±»å™¨çš„ç»“æœåŠ æƒå¾—åˆ°æœ€ç»ˆç»“æœã€‚

Boostingæ–¹æ³•é€šè¿‡èšç„¦äºåŸºåˆ†ç±»å™¨åˆ†é”™çš„æ ·æœ¬ï¼Œå‡å°‘é›†æˆåˆ†ç±»å™¨çš„åå·®ã€‚

### 1.1 Adaboost


### 1.2 GBDT
åœ¨GBDTçš„è¿­ä»£ä¸­ï¼Œå‡è®¾æˆ‘ä»¬å‰ä¸€è½®è¿­ä»£å¾—åˆ°çš„å¼ºå­¦ä¹ å™¨æ˜¯$f_{t-1}(x)$, æŸå¤±å‡½æ•°æ˜¯$L(y, f_{t-1}(x))$, æˆ‘ä»¬æœ¬è½®è¿­ä»£çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªCARTå›å½’æ ‘æ¨¡å‹çš„å¼±å­¦ä¹ å™¨â„ğ‘¡(ğ‘¥)ï¼Œè®©æœ¬è½®çš„æŸå¤±å‡½æ•°$ğ¿(ğ‘¦,ğ‘“_ğ‘¡(ğ‘¥))=ğ¿(ğ‘¦,ğ‘“_{ğ‘¡âˆ’1}(ğ‘¥)+â„_ğ‘¡(ğ‘¥))$æœ€å°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæœ¬è½®è¿­ä»£æ‰¾åˆ°å†³ç­–æ ‘ï¼Œè¦è®©æ ·æœ¬çš„æŸå¤±å°½é‡å˜å¾—æ›´å°ã€‚

GBDTåˆ©ç”¨æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦æ¥æ‹Ÿåˆæœ¬è½®æŸå¤±çš„è¿‘ä¼¼å€¼ï¼Œè¿›è€Œæ‹Ÿåˆä¸€ä¸ªCARTå›å½’æ ‘ã€‚å³ï¼š
$$ r_{ti} = - [\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}]_{f(x)=f_{t-1}(x)}$$
åˆ©ç”¨$(x_i, r_{ti})$,æˆ‘ä»¬å¯ä»¥æ‹Ÿåˆä¸€é¢—CARTå›å½’æ ‘ï¼Œå¾—åˆ°äº†ç¬¬té¢—å›å½’æ ‘ã€‚

é’ˆå¯¹æ¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹é‡Œçš„æ ·æœ¬ï¼Œæˆ‘ä»¬æ±‚å‡ºä½¿æŸå¤±å‡½æ•°æœ€å°ï¼Œä¹Ÿå°±æ˜¯æ‹Ÿåˆå¶å­èŠ‚ç‚¹æœ€å¥½çš„çš„è¾“å‡ºå€¼$c_{tj}$ï¼š
$$c_{tj} = argmin_c \sum_{x_i \in R_{tj}} L(y_i, f_{t-1}(x_i)+c)$$

### 1.3 XGBoost
> XGBoost: A Scalable Tree Boosting System

A tree ensemble model uses K additive functions to
predict the output:
$$ \hat{y}_i = \sum_{k=1}^K f_k(x_i)$$
The model is trained in an additive manner. We will need to add $f_t$ to minimize the following objective:
$$ L^{(t)} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t)$$
where second term penalize the complexity of model: $\Omega(f) =\gamma T +\frac{1}{2}\lambda ||w||^2$. 
- T: number of leaves
- w: weights in leaves

Use second-order approximation to optimize the objective:
$$ L^{(t)} \simeq \sum_{i=1}^n [l(y_i, \hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)$$
where $g_i=\partial_{\hat{y}^{(t-1)}} l(y_i, \hat{y}^{(t-1)})$ and $h_i=\partial_{\hat{y}^{(t-1)}}^2 l(y_i, \hat{y}^{(t-1)})$.

For a fixed structure q(x), we can compute the optimal weight $w_j$ of leaf j by:
$$ w_j^* = -\frac{\sum_{i\in I_j}g_i}{\sum_{i \in I_j}h_i +\lambda}$$
And the corresponding optimal loss value:
$$L^{(t)}(q) = -\frac{1}{2}\sum_{j=1}^T\frac{(\sum_{i\in I_j}g_i)^2}{\sum_{i \in I_j}h_i +\lambda} +\gamma T$$
XGBoost uses two additional techniques to further prevent overfitting:

1. shrinkage
2. feature subsampling

Split finding algorithms:
In each iteration, we need to find the best tree structure.

1. Exact Greedy Algorithm
2. Approximation Algorithm
3. Weighted Quantile
4. Sparsity-aware Split Finding


## 2. Bagging

Baggingæ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å„ä¸ªåŸºåˆ†ç±»å™¨æ— å¼ºä¾èµ–ï¼Œå¯ä»¥å¹¶è¡Œè®­ç»ƒã€‚ä¸ºäº†è®©åŸºåˆ†ç±»å™¨ä¹‹é—´äº’ç›¸ç‹¬ç«‹ï¼Œé€šå¸¸å°†è®­ç»ƒé›†åˆ†ä¸ºè‹¥å¹²å­é›†ã€‚

Baggingé€šè¿‡å¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œå¤šæ¬¡é‡‡æ ·ï¼Œè®­ç»ƒä¸åŒçš„æ¨¡å‹æ¥å‡å°‘é›†æˆåˆ†ç±»å™¨çš„æ–¹å·®ã€‚

### 2.1 Random Forest
éšæœºæ£®æ—æ˜¯baggingçš„ä¸€ä¸ªç‰¹åŒ–è¿›é˜¶ç‰ˆï¼Œæ‰€è°“çš„ç‰¹åŒ–æ˜¯å› ä¸ºéšæœºæ£®æ—çš„å¼±å­¦ä¹ å™¨éƒ½æ˜¯å†³ç­–æ ‘ã€‚æ‰€è°“çš„è¿›é˜¶æ˜¯éšæœºæ£®æ—åœ¨baggingçš„æ ·æœ¬éšæœºé‡‡æ ·åŸºç¡€ä¸Šï¼ŒåˆåŠ ä¸Šäº†ç‰¹å¾çš„éšæœºé€‰æ‹©ã€‚


## 3. ç»“åˆç­–ç•¥
1. å¹³å‡æ³•ï¼šç”¨äºå›å½’ï¼šç®—æœ¯å¹³å‡æ³•ï¼ŒåŠ æƒå¹³å‡æ³•
2. æŠ•ç¥¨æ³•ï¼šç”¨äºåˆ†ç±»ï¼šå¤šæ•°æŠ•ç¥¨æ³•ï¼ŒåŠ æƒæŠ•ç¥¨æ³•
3. å­¦ä¹ æ³•ï¼šä¸æ˜¯å¯¹å¼±å­¦ä¹ å™¨çš„ç»“æœåšç®€å•çš„é€»è¾‘å¤„ç†ï¼Œè€Œæ˜¯å†åŠ ä¸Šä¸€å±‚å­¦ä¹ å™¨ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å°†è®­ç»ƒé›†å¼±å­¦ä¹ å™¨çš„å­¦ä¹ ç»“æœä½œä¸ºè¾“å…¥ï¼Œå°†è®­ç»ƒé›†çš„è¾“å‡ºä½œä¸ºè¾“å‡ºï¼Œé‡æ–°è®­ç»ƒä¸€ä¸ªå­¦ä¹ å™¨æ¥å¾—åˆ°æœ€ç»ˆç»“æœã€‚
