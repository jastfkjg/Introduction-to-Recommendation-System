# 聚类

## k-means

```
1. 创建k个点作为初始中心，每个中心点代表一个簇
2. 对数据集中每个数据点，计算其与k个中心的距离，将其分配到距离就近的簇
3. 重新计算簇中所有点均值，并将其作为中心
4. 重复2，3步，直到达到终止条件（达到指定迭代次数，簇中心变化率足够小。。）
```

k-means的缺陷：

1. 对异常值敏感
2. 对初始中心点选择敏感
3. k值需人为选择

### k-means++
K-Means++算法在聚类中心的初始化过程中的基本原则是使得初始的聚类中心之间的相互距离尽可能远，这样可以避免出现上述的问题。从而可以解决K- Means算法对初始簇心比较敏感的问题。

## DBSCAN
https://zhuanlan.zhihu.com/p/104355127

 𝜖-邻域：对于$𝑥_𝑗∈𝐷$，其𝜖-邻域包含样本集D中与𝑥𝑗的距离不大于𝜖的子样本集，这个子样本集的个数记为|𝑁𝜖(𝑥𝑗)|。

 核心对象：对于任一样本𝑥𝑗∈𝐷，如果其𝜖-邻域对应的𝑁𝜖(𝑥𝑗)至少包含M个样本，即如果|𝑁𝜖(𝑥𝑗)|≥𝑀，则𝑥𝑗是核心对象。

 密度直达：如果𝑥𝑖位于𝑥𝑗的𝜖-邻域中，且𝑥𝑗是核心对象，则称𝑥𝑖由𝑥𝑗密度直达。

 密度可达：对于𝑥𝑖和𝑥𝑗,如果存在样本样本序列𝑝1,𝑝2,...,𝑝𝑇,满足𝑝1=𝑥𝑖,𝑝𝑇=𝑥𝑗, 且𝑝𝑡+1由𝑝𝑡密度直达，则称𝑥𝑗由𝑥𝑖密度可达。

 密度相连：对于𝑥𝑖和𝑥𝑗,如果存在核心对象样本𝑥𝑘，使𝑥𝑖和𝑥𝑗均由𝑥𝑘密度可达，则称𝑥𝑖和𝑥𝑗密度相连。密度相连关系是满足对称性的。

 DBSCAN的聚类定义：由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。

 任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止。

一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。

## GMM

高斯混合模型 假设每个簇的数据都符合高斯分布，当前数据呈现出的分布就是各个簇的高斯分布叠加在一起的结果。

高斯混合模型的公式为：
$$p(x) = \sum_i^K \alpha_i N(x|\mu_i, \Sigma_i) $$

1. E步： 依据当前参数，计算每个数据j来自子模型k的可能性:
   $$ \gamma_{jk} = \frac{\alpha_k N(x_j|\mu_k, \Sigma_k)}{\sum_k^K \alpha_k N(x_j|\mu_k, \Sigma_k)}$$
2. M步： 计算新一轮迭代的模型参数：
   $$ \mu_k = \frac{\sum_j^N \gamma_{jk}x_j}{\sum_j^N \gamma_{jk}}$$
   $$ \Sigma_k = \frac{\sum_j^N \gamma_{jk}(x_j -\mu_k)(x_j-\mu_k)^T}{\sum_j^N \gamma_{jk}}$$
   $$ \alpha_k = \frac{\sum_j^N \gamma_{jk}}{N}$$
3. 重复E，M步直至收敛。

### EM算法简介

当我们得到的观察数据包含的隐含数据，此时我们未知的有隐含数据和模型参数，因而无法直接用极大化对数似然函数得到模型分布的参数。

EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含数据（EM算法的E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。因此基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。

在k-means和GMM中，隐含数据是每个数据点属于某个簇，只不过k-means是确定性的，而GMM则是计算每个点属于各个簇的概率。模型参数在k-means中对应的是簇的中心点坐标，而在GMM中则是各个簇的均值，方差以及权重。

## 层次聚类

### Aggloerative 层次聚类

### Divisive 层次聚类