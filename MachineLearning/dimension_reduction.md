# 降维

## PCA

PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。

协方差可以表示两个变量的相关性。协方差公式为：
$$ cov(X, Y)=\frac{1}{n-1}E[(X-E(X))(Y-E(Y))]$$
若变量均值为0，则：
$$ cov(X,Y) = \frac{1}{n-1}EXY$$

### **特征值分解**：（只适用于方阵）
$$Av = \lambda v$$
v是矩阵A的特征向量, λ是特征向量v对应的特征值。

特征值分解，就是将矩阵A分解为如下式：
$$ A=Q\Sigma Q^{-1} $$

其中，Q是矩阵A的特征向量组成的矩阵，$\Sigma$则是一个对角阵，对角线上的元素就是特征值。一般我们会将组成Q的特征向量标准化，即满足 $||w_i||=1$, 此时Q的n个特征向量为标准正交基，满足 $Q^TQ=I$，即 $Q^T=Q^{-1}$，也就是说Q为酉矩阵。此时，特征值分解可以写成：

$$ A=Q\Sigma Q^T $$


### **奇异值分解（SVD）**：（能适用于任意矩阵）

$$ A=U\Sigma V^T$$
假设A是一个m*n的矩阵，那么得到的U是一个m*m的方阵，U里面的正交向量被称为左奇异向量。Σ是一个m*n的矩阵，Σ除了对角线其它元素都为0，对角线上的元素称为奇异值。$V^T$是V的转置矩阵，是一个n*n的矩阵。

U和V都是酉矩阵，即$U^TU=I, V^TV=I$。

证明：
$$ A=U\Sigma V^T $$
$$A^T = V\Sigma^TU^T $$
$$AA^T = U\Sigma \Sigma^T U^T = U\Sigma^2U^T$$
$$A^TA = V\Sigma^2V^T$$

可以看出，$AA^T$的特征向量组成SVD中的U矩阵，$A^TA$的特征向量组成SVD中的V矩阵。同时，还可以看出特征值矩阵等于奇异值矩阵的平方，若$A$的奇异值为$\sigma_i$，$AA^T, A^TA$的非零特征值为$\lambda_i$，则有：
$$ \sigma_i = \sqrt{\lambda_i} $$

注: $AA^T, A^TA$的非零特征值是相同的：
$$ A^TAx=\lambda x $$
$$ AA^T (Ax) = A(A^TAx) = A\lambda x = \lambda (Ax)$$


### 基于特征值分解协方差矩阵实现PCA

数据$X={x_1, x_2, ..., x_n}, dim(x_i)=m$， 需降至k维：
1. 去平均值(即去中心化)，即每一位特征减去各自的平均值。

2. 计算协方差矩阵 $\frac{1}{n}XX^T$,注：这里除或不除样本数量n或n-1,其实对求出的特征向量没有影响。

3. 用特征值分解方法求协方差矩阵$\frac{1}{n}XX^T$ 的特征值与特征向量。

4. 对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为行向量组成特征向量矩阵P。

5. 将数据转换到k个特征向量构建的新空间中，即Y=PX。

### 基于奇异值分解矩阵实现PCA

直接求解X的奇异值分解，实际上同上一方法相同，因为奇异值分解可以通过借助$XX^T$的特征值分解来实现。

但是，当样本维度很高时，协方差矩阵计算效率低，而SVD 除了特征值分解这种求解方式外，还有更高效的迭代求解方式，避免了协方差的计算。


## LDA (Linear Discriminant Analysis)

是一种有监督的线性降维算法, LDA是为了使得降维后的数据点尽可能地容易被区分。目标：最小化类内距离和最大化类间距离。

## LLE (Locally Linear Embedding)
LLE 是一种非线性降维算法，它能够使降维后的数据较好地保持原有 流形结构。
LLE算法认为每一个数据点都可以由其近邻点的线性加权组合构造得到。算法的主要步骤分为三步：
1. 寻找每个样本点的k个近邻点
2. 由每个样本点的近邻点计算出该样本点的局部重建权值矩阵
3. 由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值

